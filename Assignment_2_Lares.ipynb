{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91a5afd5-2211-4079-a669-0143a4b1a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 2 for ELEE6280 - by Oscar Lares\n",
    "\n",
    "# This is my previous code from Assignment 1 with updates made so RANSAC is used to eliminate outliers\n",
    "# Some parts of code adapted from these 2 sources: \n",
    "# https://medium.com/data-breach/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cf\n",
    "# https://github.com/tshanmukh/Facial-KeyPoint-Detection/blob/master/ORB.ipynb\n",
    "\n",
    "# Submitted 4/28/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c26c70-ae04-4e80-9db8-a69d0a778761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2 and os module needed\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db2f57e-9d76-4a8b-b1b5-99cbc2bc4625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying folder path and extension for where images are stored\n",
    "folder_path = 'D:/OneDrive - University of Georgia/School/Classes/ELEE6280 - Intro Robotics/Assignment 2/kitti_Seq'\n",
    "extension = '.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b659d6-8e62-4ef6-84a4-3e205d7a6cca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load images into an array, use os.listdir to access folder path and read images in loop\n",
    "images = []\n",
    "for filename in sorted(os.listdir(folder_path)):\n",
    "    if filename.endswith(extension):\n",
    "        image = cv2.imread(os.path.join(folder_path, filename), cv2.IMREAD_COLOR)\n",
    "        images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3c55aeb4-2cbf-456d-bda4-8e6bf23afafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create SIFT feature detector and find SIFT keypoints and descriptors for first image\n",
    "sift = cv2.SIFT_create()\n",
    "kp, desc = sift.detectAndCompute(images[0], None)\n",
    "\n",
    "#create a Brute Force Based matcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "#initialize total keypoint and matched keypoints to help keep track of accuracy\n",
    "total_kp = 0\n",
    "matched_kp = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "40765671-939f-4b72-9ff6-4d21ee8dac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create videowriter object to save tracking results to video format\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "trajectory_writer = cv2.VideoWriter('trajectory_video_lares.avi', fourcc, 5, images[0].shape[::-1][1:3], True)\n",
    "point_cloud_writer = cv2.VideoWriter('point_cloud_video_lares.avi', fourcc, 5, images[0].shape[::-1][1:3], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "06465fbf-378b-476b-b9fc-7bd2ac50a455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First F matrix: [[ 5.73467136e-07  8.30771836e-04 -1.54269193e-01]\n",
      " [-8.30379187e-04 -1.11762742e-06  4.78575331e-01]\n",
      " [ 1.53414730e-01 -4.82361789e-01  1.00000000e+00]]\n",
      "First E matrix: [[ 2.86686952e-01  4.15318387e+02 -2.80201690e+00]\n",
      " [-4.15122094e+02 -5.58722859e-01 -1.64339682e+01]\n",
      " [ 2.73785539e+00  1.36391831e+01  1.60200390e-02]]\n"
     ]
    }
   ],
   "source": [
    "#create loop for remaining images after getting kp and desc for first one in order to track the features\n",
    "\n",
    "#initialize trajectory and pointcloud map variables to draw on\n",
    "TrajectoryMap = np.zeros((1000, 1000, 3), dtype=np.uint8)\n",
    "PointCloudMap = np.zeros((1000, 1000, 3), dtype=np.uint8)\n",
    "scaling_factor_traj = 300\n",
    "scaling_factor_cloud = 4\n",
    "\n",
    "frameNumber=1\n",
    "\n",
    "for i in range(1, len(images)):\n",
    "    \n",
    "    #find SIFT keypoints and descriptors for image 'i'\n",
    "    kp2, desc2 = sift.detectAndCompute(images[i], None)\n",
    "    \n",
    "    #perform the matching between the SIFT descriptors of the current image and the previous image\n",
    "    matches = bf.match(desc, desc2)\n",
    "      \n",
    "    # apply RANSAC to remove outliers\n",
    "    pts1 = np.float32([kp[m.queryIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "    \n",
    "    #Camera intrinsic matrix, given in the homework document\n",
    "    K = np.array([[707.0493, 0.0, 604.0814], [0.0, 707.0493, 180.5066], [0.0, 0.0, 1.0]])\n",
    "    \n",
    "    #Find Fundamental Matrix\n",
    "    F, mask = cv2.findFundamentalMat(pts1, pts2, cv2.FM_RANSAC, 0.1, 0.99)\n",
    "    if frameNumber == 1:\n",
    "        print(\"First F matrix:\", F)\n",
    "    if F is None:\n",
    "        continue\n",
    "    \n",
    "    #Find Essential matrix from F and from given K\n",
    "    try:\n",
    "        E = K.T.dot(F.dot(K))\n",
    "    except:\n",
    "        continue\n",
    "    if frameNumber == 1:\n",
    "        print('First E matrix:', E)\n",
    "    if E is None:\n",
    "        continue\n",
    "    \n",
    "    #Recover the post (R and t) from Essential matrix\n",
    "    points, R, t, mask = cv2.recoverPose(E, pts1, pts2, K)\n",
    "    \n",
    "    # If translation is small or backwards, ingore, b/c car is stopped so don't transform tracking or rotation gets out-of-whack\n",
    "    if t[2][0] > 0 or abs(t[2][0]) < 0.2: \n",
    "        continue\n",
    "    if t[2][0] <= 1:\n",
    "        currentR = R\n",
    "        currentT = t.T\n",
    "        currentRt = np.append(currentR, currentT.T, axis=1).T\n",
    "    else:              \n",
    "        currentRt = np.append(R, t.T, axis=1).T\n",
    "        t_homog = np.append(t, np.array([[1]]), axis=0)            \n",
    "        currentT = currentRt.T.dot(t_homog).T                        \n",
    "        currentR = R.dot(currentR)\n",
    "    \n",
    "    M1 = np.hstack((R, t))\n",
    "    M2 = np.hstack((np.eye(3, 3), np.zeros((3, 1))))\n",
    "\n",
    "    P1 = np.dot(K,  M2)\n",
    "    P2 = np.dot(K,  M1)\n",
    "    point_4d_hom = cv2.triangulatePoints(P1, P2, np.float32(pts1), np.float32(pts2))       \n",
    "    Points3D = []\n",
    "    Points3D_Colors = []\n",
    "    for index, point in enumerate(point_4d_hom.T):\n",
    "        point = point[:4]/point[-1]\n",
    "        point[0] = -point[0]            # Reverse X direction\n",
    "        # Filter points            \n",
    "        distance = math.sqrt(math.pow(point[0], 2) + math.pow(point[1], 2) + math.pow(point[2],2))\n",
    "        if distance > 75 or distance < 5 or point[2] > 0:\n",
    "            Point3D = np.array([0, 0, 0])\n",
    "        else:               \n",
    "            # NEXT TRY adding currentT and doing the currentR to each point  \n",
    "            Point3D = currentRt.T.dot(point)                   \n",
    "            if pts1[index][0][1] < images[i].shape[0] and pts1[index][0][0] < images[i].shape[1]:\n",
    "                Points3D.append(Point3D)               \n",
    "                color = images[i][int(pts1[index][0][1]), int(pts1[index][0][0]), :]\n",
    "                Points3D_Colors.append(color)\n",
    "\n",
    "    #Draw the map, starting at center (500,500)\n",
    "    xCoord_traj = int(currentT[0][0] * scaling_factor_traj + 500) #currentT\n",
    "    yCoord_traj = int(currentT[0][2] * scaling_factor_traj + 500) #currentT\n",
    "    point = (xCoord_traj, yCoord_traj)\n",
    "    cv2.circle(TrajectoryMap, point, 3, (255,0,0), -1)\n",
    "\n",
    "    # Draw 3D point cloud on 2D map top-down\n",
    "    for index, point in enumerate(Points3D):\n",
    "        xCoord = int(point[0] * scaling_factor_cloud + 500)\n",
    "        yCoord = int(point[2] * scaling_factor_cloud + 500)\n",
    "        cloudpoint = (xCoord, yCoord)\n",
    "        color = tuple ([int(x) for x in Points3D_Colors[index]])\n",
    "        cv2.circle(PointCloudMap, cloudpoint, 3, color, 1)\n",
    "    \n",
    "    cv2.imshow(\"Point Cloud Map\", PointCloudMap)    \n",
    "    cv2.imshow(\"Trajectory Map\", TrajectoryMap)\n",
    "    trajectory_writer.write(TrajectoryMap)\n",
    "    point_cloud_writer.write(PointCloudMap)\n",
    "    \n",
    "    # Add a small delay and a keyboard interrupt check\n",
    "    key = cv2.waitKey(2) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "    #update kp and desc for the next iteration in the loop\n",
    "    kp, desc = kp2, desc2\n",
    "\n",
    "    frameNumber=0\n",
    "\n",
    "#end the video writer object and destroy all windows\n",
    "trajectory_writer.write(TrajectoryMap)\n",
    "point_cloud_writer.write(PointCloudMap)\n",
    "trajectory_writer.release()\n",
    "point_cloud_writer.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
